{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52278c7a-87ca-420e-9515-963250424c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages jakac:spark-python-knn:0.0.3\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ed6791-8d52-41e8-9357-365ba2798e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export SPARK_HOME=~/.virtualenvs/pyspark/lib/python3.8/site-packages/pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e559b0-a0c1-45cf-8e60-37aa30e69015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gaussalgo.knn import compute_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75eea60a-4d7a-4f46-a8f8-fa95f1e77ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyspark.sql.functions import lower, col, udf, isnan, when, count, row_number\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from  pyspark.sql.types import StringType\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import cv2\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d475b18b-a616-41a9-82e5-0dab0f4cedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9da0a59-a296-4877-ad4c-11520d83dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the path to the images and properties\n",
    "PATH = '../datasets/'\n",
    "MODEL_PATH = \"./models/efficientnetb5_notop.h5\"\n",
    "\n",
    "image_path = PATH +\"train_images/\"\n",
    "IMG_SIZE = 456 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c11a65-76ff-4bdc-98aa-e4338b136ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- posting_id: string (nullable = true)\n",
      " |-- image: string (nullable = true)\n",
      " |-- image_phash: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- label_group: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_rdd = spark.read.format(\"csv\").option(\"header\",\"true\").load(PATH + \"train.csv\")\n",
    "train_rdd = train_rdd.withColumn(\"id\", monotonically_increasing_id())\n",
    "train_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7ca3ad-1f5c-456e-9061-6f429bb9b64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===========================================================(1 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+-----+-----------+---+\n",
      "|posting_id|image|image_phash|title|label_group| id|\n",
      "+----------+-----+-----------+-----+-----------+---+\n",
      "|         0|    0|          0|    0|          4|  0|\n",
      "+----------+-----+-----------+-----+-----------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_rdd.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_rdd.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e920bf-7872-4f43-9933-aa9959eb5215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label_group='249114794')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.select(\"label_group\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9224e2-0fa4-41c4-97b8-57f59160728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_rdd.na.fill(value=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9669a3-2caf-4bd4-b66d-8a1307a2b1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+-----+-----------+---+\n",
      "|posting_id|image|image_phash|title|label_group| id|\n",
      "+----------+-----+-----------+-----+-----------+---+\n",
      "|         0|    0|          0|    0|          0|  0|\n",
      "+----------+-----+-----------+-----+-----------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_rdd.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_rdd.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c644704-a6ea-4337-8495-89eda9a64693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = train_rdd.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "# train['target'] = train.label_group.map(tmp)\n",
    "# traindf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "   # ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54dd767e-d022-4129-958f-57b85233c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_replace_multispace_by_space = udf(lambda text: re.sub('\\s+', ' ',StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f96c3f48-635f-40eb-9c40-fd0d1670a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_rdd.withColumn('title', lower(col('title')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2cda5b-c1f1-4f9b-b607-f138493f76e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokenizer = Tokenizer().setInputCol(\"title\").setOutputCol(\"words\")\n",
    "train_rdd = tokenizer.transform(train_rdd)\n",
    "\n",
    "# vectorize\n",
    "vectorizer = CountVectorizer(inputCol='words', outputCol='vectorizer').fit(train_rdd)\n",
    "train_rdd = vectorizer.transform(train_rdd)\n",
    "\n",
    "# calculate scores\n",
    "idf = IDF(inputCol=\"vectorizer\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(train_rdd)\n",
    "train_rdd = idf_model.transform(train_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd545245-3109-4497-8f89-08aa51c75755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- posting_id: string (nullable = false)\n",
      " |-- image: string (nullable = false)\n",
      " |-- image_phash: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- label_group: string (nullable = false)\n",
      " |-- id: long (nullable = false)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_rdd = train_rdd.drop(col(\"words\"))\n",
    "train_rdd= train_rdd.drop(col(\"vectorizer\"))\n",
    "train_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2294a81c-927f-4428-9938-57885165de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, df, img_size=IMG_SIZE, batch_size=32, path=''): \n",
    "        self.df = df\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path\n",
    "        self.indexes = np.arange( self.df.count() )\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        ct = self.df.count() // self.batch_size\n",
    "        ct += int(( (self.df.count()) % self.batch_size)!=0)\n",
    "        return ct\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = self.__data_generation(indexes)\n",
    "        return X\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples'\n",
    "        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n",
    "        start = int(min(indexes))\n",
    "        end = int(max(indexes))\n",
    "        df = self.df.where(col(\"id\").between(start, end))\n",
    "        for i, row in enumerate(df.select(\"image\").collect()):\n",
    "            img = cv2.imread(self.path + row.image)\n",
    "            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "081c5111-2302-4896-93db-762c94173851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(data_partition):\n",
    "    WGT = \"/home/joker/Workspace/Playground/retail_duplicate/deep_deduplicate/efficient_knn/trained_models/efficientnetb5_notop.h5\"\n",
    "    model = tf.keras.applications.efficientnet.EfficientNetB5(weights=WGT, input_shape=None, include_top=False,\n",
    "                                                                pooling=\"avg\",\n",
    "                                                                drop_connect_rate=0.2)\n",
    "\n",
    "    data = DataGenerator(data_partition, path=image_path)\n",
    "\n",
    "    image_embedding = model.predict(data)\n",
    "    return image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d81709-21d8-4274-a607-8734b7053680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 22:54:41.271312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:41.295129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:41.295827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:41.296909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-13 22:54:41.297483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:41.298235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:41.298831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:47.940743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:47.942239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:47.943337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 22:54:47.944246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3364 MB memory:  -> device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "DataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "DataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 22:54:59.897247: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n",
      "2022-07-13 22:55:01.047890: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.28GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-07-13 22:55:04.159130: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-07-13 22:55:04.348010: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.61GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 4:15DataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 2/32 [>.............................] - ETA: 3s  DataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 3/32 [=>............................] - ETA: 29sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 4/32 [==>...........................] - ETA: 36sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 5/32 [===>..........................] - ETA: 39sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 6/32 [====>.........................] - ETA: 39sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 7/32 [=====>........................] - ETA: 39sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 8/32 [======>.......................] - ETA: 39sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      " 9/32 [=======>......................] - ETA: 38sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "10/32 [========>.....................] - ETA: 37sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "11/32 [=========>....................] - ETA: 36sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "12/32 [==========>...................] - ETA: 34sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "13/32 [===========>..................] - ETA: 33sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "14/32 [============>.................] - ETA: 31sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "15/32 [=============>................] - ETA: 30sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "16/32 [==============>...............] - ETA: 28sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "17/32 [==============>...............] - ETA: 26sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "18/32 [===============>..............] - ETA: 25sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "19/32 [================>.............] - ETA: 23sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "20/32 [=================>............] - ETA: 21sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "21/32 [==================>...........] - ETA: 19sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "22/32 [===================>..........] - ETA: 18sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "23/32 [====================>.........] - ETA: 16sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "24/32 [=====================>........] - ETA: 14sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "25/32 [======================>.......] - ETA: 12sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "26/32 [=======================>......] - ETA: 10sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "27/32 [========================>.....] - ETA: 9s DataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "28/32 [=========================>....] - ETA: 7sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "29/32 [==========================>...] - ETA: 5sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "30/32 [===========================>..] - ETA: 3sDataFrame[posting_id: string, image: string, image_phash: string, title: string, label_group: string, id: bigint, tfidf_features: vector]\n",
      "31/32 [============================>.] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 22:56:03.174051: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.41GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-07-13 22:56:03.279803: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 67s 2s/step\n"
     ]
    }
   ],
   "source": [
    "res = make_prediction(train_rdd.where(col(\"id\").between(0, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6bb5af4-d8fa-49cd-bf8c-cfba8f24fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_features(indx):\n",
    "    print(indx)\n",
    "    return Vectors.dense(res[indx-1]) # since row num begins from 1\n",
    "ud_f = F.udf(add_image_features,VectorUDT())\n",
    "df_image = train_rdd.where(col(\"id\").between(0, 999)).withColumn(\"image_features\",ud_f(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6a62249-8fea-48dc-8a5a-8bebfc58b3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- posting_id: string (nullable = false)\n",
      " |-- image: string (nullable = false)\n",
      " |-- image_phash: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- label_group: string (nullable = false)\n",
      " |-- id: long (nullable = false)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      " |-- image_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_image.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d2795c1-102b-4049-b6e1-754c053727ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(image='0000a68812bc7e98c42888dfb1c07da0.jpg')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image.select(\"image\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cc4c2e3-f02a-4ebd-8a58-fc6217d8df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(a, b):\n",
    "    np.sqrt(np.sum((a - b) ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8e07de6-b590-469a-8a95-5ee4f09d4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter(row):\n",
    "    print(row)\n",
    "    distances = df_image.map(lambda x: euclidian_distance(row, x))\n",
    "    print(distances)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0161f5a-b837-4804-b74a-f02ba3f781c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = df_image.select(\"tfidf_features\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "875043d1-96e2-4888-8700-4b719c48504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfeatures = text_features[\"tfidf_features\"].apply(lambda x : np.array(x.toArray())).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e30135f4-0ef8-42eb-a576-dcb7d85b6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = NearestNeighbors(n_neighbors=6, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e07c0a8f-174f-49a2-bdcc-a994fe9f9bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(tfeatures)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89761da7-edb4-4e5a-a1a0-e4746a0e23c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mknn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/pyspark/lib/python3.8/site-packages/sklearn/neighbors/_unsupervised.py:168\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/pyspark/lib/python3.8/site-packages/sklearn/neighbors/_base.py:444\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[0;32m--> 444\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_algorithm_metric()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/pyspark/lib/python3.8/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.virtualenvs/pyspark/lib/python3.8/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "knn_model.fit(tfeatures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
